{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what we want to do in this notebook is create a confusion matrix for each feature, for each dataset, for each model\n",
    "- Steps:\n",
    "    - Split the datasets\n",
    "- Create confusion matrix per feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"\\data\\final_bias_data\\20240203_bias_lhr_processed.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA + global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in df.columns:\n",
    "#     print(column)\n",
    "#     print(df[column].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_features = ['geslacht', # Gender\n",
    "                 'is_parttime_parent', 'is_fulltime_parent', # Parenthood\n",
    "                 'Leeftijd<30', 'Leeftijd<40', 'Leeftijd<50', # Age\n",
    "                 'IsNederlands', 'IsWesters'] # Nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in bias_features:\n",
    "    print(feature)\n",
    "    print(df[feature].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# privileged_unprivileged = {'geslacht' : {'privileged' : 'M',\n",
    "#                                          'unprivileged' : 'V'}, # Gender\n",
    "#                  'is_parttime_parent': {'privileged' : True,\n",
    "#                                          'unprivileged' : False}, # Parenthood\n",
    "#                  'is_fulltime_parent': {'privileged' : True,\n",
    "#                                          'unprivileged' : False}, \n",
    "#                  'Leeftijd<30': {'privileged' : 1,\n",
    "#                                          'unprivileged' : 0}, # Age\n",
    "#                  'Leeftijd<40': {'privileged' : 1,\n",
    "#                                          'unprivileged' : 0}, \n",
    "#                  'Leeftijd<50': {'privileged' : 1,\n",
    "#                                          'unprivileged' : 0}, \n",
    "#                  'IsNederlands': {'privileged' : 1,\n",
    "#                                          'unprivileged' : 0}, # Nationality\n",
    "#                  'IsWesters': {'privileged' : 1,\n",
    "#                                          'unprivileged' : 0}\n",
    "#                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_features_values = {'geslacht' : ['M', 'V'], # Gender\n",
    "                 'IsFulltimeParent':[1, 0], # Parenthood\n",
    "                 'IsParttimeParent': [1, 0], \n",
    "                 'Leeftijd<30': [1, 0], # Age\n",
    "                 'Leeftijd<40': [1, 0],\n",
    "                 'Leeftijd<50': [1, 0], \n",
    "                 'IsNederlands': [1, 0], # Nationality\n",
    "                 'IsWesters': [1, 0]\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = {'model_before_reweighing_prediction' : 'BR', \n",
    "                     'model_after_reweighing_prediction': 'AR'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run in to issies with underscores when creating the resulting df, so we remove those where needed\n",
    "df.loc[df.dataset == 'Training_test', 'dataset'] = 'TrainingTest'\n",
    "df.loc[df.dataset == 'Training_train', 'dataset'] = 'TrainingTrain'\n",
    "\n",
    "df = df.rename(columns = {'is_fulltime_parent':'IsFulltimeParent',\n",
    "                     'is_parttime_parent': 'IsParttimeParent'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All our binary columns are in 1 and 0, not in boolean\n",
    "df.loc[df['IsFulltimeParent'] == True, 'IsFulltimeParent'] = 1\n",
    "df.loc[df['IsFulltimeParent'] == False, 'IsFulltimeParent'] = 0\n",
    "df.loc[df['IsParttimeParent'] == True, 'IsParttimeParent'] = 1\n",
    "df.loc[df['IsParttimeParent'] == False, 'IsParttimeParent'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred_columns = ['BR', 'AR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have multiple ways of labeling, make uniform\n",
    "dict_map_label = {'Onderzoekswaardig': 1,\n",
    "                    'Niet onderzoekswaardig': 0}\n",
    "\n",
    "df['Label'] = df['Label'].replace(dict_map_label)\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lets do some psuedo code for our structure\n",
    "# def calculate_CM(\n",
    "#     data: pd.DataFrame = df,\n",
    "#     feature: str,\n",
    "#     feature_value\n",
    "#     ) -> list:\n",
    "#     \"\"\"\n",
    "#     Creates the confusion matrix for the feature, feature_value pair\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         data\n",
    "#             Data to be analyzed.\n",
    "#         feature\n",
    "#             The feature we want to calculate the CMs for.\n",
    "#         feature_values\n",
    "#             The feature values we want to calculate the CMs for.\n",
    "#             We don't want to do this for all values of the features since there are 'Onbekend' and additional values.\n",
    "#             We want binary grouping.\n",
    "\n",
    "#         Returns:\n",
    "#         Confusion Matrix as an (2, 2) array with order tn, fp, fn, tp\n",
    "#     \"\"\"\n",
    "   \n",
    "#     df_temp = df.loc[df[feature] == value]\n",
    "#     CM_feature_value = confusion_matrix()\n",
    "        \n",
    "        \n",
    "#     return CM_dict\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ugly nested for-loop galore for quick and dirty, maybe rewrite\n",
    "# Create dict of CMs for each combination of dataset, model, feature, and feature value\n",
    "CM_dict = {}\n",
    "\n",
    "for dataset in df['dataset'].unique():\n",
    "    print(dataset)\n",
    "    df_temp = df.loc[df['dataset'] == dataset]\n",
    "    for model in model_pred_columns:\n",
    "        for feature in bias_features_values.keys():\n",
    "            for feature_value in bias_features_values[feature]:\n",
    "                df_CM = df_temp.loc[df_temp[feature] == feature_value]\n",
    "                # if df_temp.shape != (0,30):\n",
    "                #     # display(df_temp)\n",
    "                CM_feature_value_model = confusion_matrix(df_CM['Label'], df_CM[model])\n",
    "                CM_dict[f\"{dataset}_{model}_{feature}_{feature_value}\"] = CM_feature_value_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary for the long data format\n",
    "long_data_dict = {\n",
    "    'Dataset': [],\n",
    "    'Model': [],\n",
    "    'Feature': [],\n",
    "    'Feature_Value': [],\n",
    "    'Metric': [],\n",
    "    'Value': []\n",
    "}\n",
    "\n",
    "for key, value in CM_dict.items():\n",
    "    # The key contains all info we need on dataset, model, feature, and feature value, so we split\n",
    "    parts = key.split('_')\n",
    "    # Get the info\n",
    "    dataset = parts[0]\n",
    "    model = parts[1]\n",
    "    feature = parts[2]\n",
    "    feature_value = parts[3]\n",
    "    # metric_values is (2,2) matrix, we want a list\n",
    "    metric_values = [val for sublist in value for val in sublist]\n",
    "    # Check if any metric value is lower than 10\n",
    "    if any(val < 10 for val in metric_values):\n",
    "        # We don't communicate numbers below 10, so remove those\n",
    "        metric_values = [0 if val < 10 else val for val in metric_values]\n",
    "        # If there is only one metric value below 10, the other three values can be calculated based on group size\n",
    "        # So then we remove the two lowest values\n",
    "        if sum(val < 10 for val in metric_values) == 1:\n",
    "            min_values = sorted(metric_values)[:2]\n",
    "            metric_values = [0 if val in min_values else val for val in metric_values]\n",
    "    # Add each metric value along with other information to the long data dictionary\n",
    "    # The order of metrics seems unintuitive but is in accordance to the output from sklearn confmatrix\n",
    "    for metric, metric_value in zip([\"TN\", \"FP\", \"FN\", \"TP\"], metric_values):\n",
    "        long_data_dict['Dataset'].append(dataset)\n",
    "        long_data_dict['Model'].append(model)\n",
    "        long_data_dict['Feature'].append(feature)\n",
    "        long_data_dict['Feature_Value'].append(feature_value)\n",
    "        long_data_dict['Metric'].append(metric)\n",
    "        long_data_dict['Value'].append(metric_value)\n",
    "\n",
    "# Convert the long data dictionary to a DataFrame\n",
    "df_cms = pd.DataFrame(long_data_dict)\n",
    "\n",
    "display(df_cms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot = df.loc[df['dataset'] == 'Pilot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cms_under10 = df_cms.loc[df_cms['Value'] < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_dict['Prepilot_AR_IsNederlands_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cms.to_excel(\"\\bias_analysis\\20240228_CMs_LHR_SlimmeCheck.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create count for most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.0f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belangrijkste_feature_columnnames = ['Belangrijkste feature 1', 'Belangrijkste feature 2', 'Belangrijkste feature 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want counts so we remap the geslacht column to values\n",
    "dict_map_label = {'M': 1,\n",
    "                    'V': 0}\n",
    "\n",
    "df['geslacht'] = df['geslacht'].replace(dict_map_label)\n",
    "df['geslacht'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the groupby we need to change string dtype to floats in order to sum\n",
    "for column in bias_features_values.keys():\n",
    "    print(df[column].unique())\n",
    "    df.loc[~df[column].isin([1, 0]), column] = np.nan\n",
    "    print(df[column].unique())\n",
    "    df[column] = df[column].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We again need a split\n",
    "df_fimp_pilot = df.loc[df['dataset'] == 'Pilot']\n",
    "df_fimp_prepilot = df.loc[(df['dataset'] == 'Prepilot')\n",
    "                          # The most important features are only known for part of the prepilot dataset, so we only add those\n",
    "                          & (df['Belangrijkste feature 1'] != 'Onbekend')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a function to calculate the count of each of the \"most important feature\" communicated to HH.\n",
    "# Here we disregard the order (because the group sizes would become to small for sharing with LHR)\n",
    "def get_imp_feature_counts(df: pd.DataFrame, \n",
    "                           belangrijkste_features: list = []\n",
    "                           ) -> pd.DataFrame:\n",
    "    \n",
    "    for column in belangrijkste_feature_columnnames:\n",
    "        belangrijkste_features.extend(list(df_fimp_prepilot[column].unique()))\n",
    "    \n",
    "    belangrijkste_features = list(set(belangrijkste_features))\n",
    "    \n",
    "    df_imp_feature_counts = pd.DataFrame(columns = ['Feature', 'Value', 'Important Feature', 'Count'])\n",
    "    df_imp_feature_counts['Important Feature'] = belangrijkste_features\n",
    "    \n",
    "    df_temp_values = pd.DataFrame(columns = ['Feature', 'Value', 'Important Feature', 'Count'])\n",
    "    df_temp_values['Important Feature'] = belangrijkste_features\n",
    "    \n",
    "    for column in bias_features_values.keys():\n",
    "        for value in [0,1]:\n",
    "            df_temp = df.loc[df[column] == value]\n",
    "            stacked = df_temp[belangrijkste_feature_columnnames].stack()\n",
    "            counts = stacked.value_counts()\n",
    "            df_temp_values['Feature'] = column\n",
    "            df_temp_values['Value'] = value\n",
    "            for imp_feature in counts.index:\n",
    "                df_temp_values.loc[df_temp_values['Important Feature'] == imp_feature, 'Count'] = counts[imp_feature]\n",
    "            df_imp_feature_counts = pd.concat([df_imp_feature_counts, df_temp_values])\n",
    "            \n",
    "    df_imp_feature_counts = df_imp_feature_counts.loc[~df_imp_feature_counts['Feature'].isna()]\n",
    "    \n",
    "    df_imp_feature_counts = df_imp_feature_counts.fillna(0)\n",
    "    df_imp_feature_counts.loc[df_imp_feature_counts['Count'] < 10, 'Count'] = 0    \n",
    "    \n",
    "    return df_imp_feature_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fimp_pilot_counts = get_imp_feature_counts(df_fimp_pilot)\n",
    "df_fimp_prepilot_counts = get_imp_feature_counts(df_fimp_prepilot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fimp_pilot_counts['dataset'] = 'Pilot'\n",
    "df_fimp_prepilot_counts['dataset'] = 'Prepilot'\n",
    "\n",
    "df_fimp_counts = pd.concat([df_fimp_pilot_counts, df_fimp_prepilot_counts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fimp_counts.to_excel(\"\\bias_analysis\\20240308_Important_Features_Counts.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
