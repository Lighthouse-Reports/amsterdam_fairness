{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final data preprocessing - Bias Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load datasets\n",
    "df_training = pd.read_excel(\"\\data\\processed_bias_data\\20240131_training_processed.xlsx\")\n",
    "df_pilot = pd.read_excel(\"\\data\\processed_bias_data\\20240131_pilot_processed.xlsx\")\n",
    "df_prepilot = pd.read_excel(\"\\data\\processed_bias_data\\20240131_prepilot_processed.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs = [df_training, df_prepilot, df_pilot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in list_dfs:\n",
    "#     display(df.columns)\n",
    "#     # display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the columns of the dataset used in the bias analysis of the pilot, plus 'is_parttime_parent' and 'is_fulltime_parent'\n",
    "# This can be found in C:\\Users\\berker007\\OneDrive - Gemeente Amsterdam\\Team AA\\Projecten\\Slimme Check\\Bias Analyse\\Code & Documentatie Bias Analyse Pilot\\Bias_analyse_code_Loek_v2\\preprocessing.ipynb\n",
    "\n",
    "bias_columns = ['Index', \n",
    "                'SlimmeCheck.PREDICTION', \n",
    "                'Slimme Check', 'IO NIO',\n",
    "       'Sherlock.Resultaat onderzoek', \n",
    "       'Leeftijd', \n",
    "       'Bias.Nationaliteit',\n",
    "       'Bias.Geslacht', \n",
    "       'SlimmeCheck.SCORE',\n",
    "       'SlimmeCheck.VALUE_AFSPRAKEN_NO_CONTACT_COUNT_LAST_YEAR',\n",
    "       'SlimmeCheck.VALUE_APPLIED_FOR_SAME_PRODUCT_LAST_YEAR',\n",
    "       'SlimmeCheck.VALUE_TOTAL_VERMOGEN',\n",
    "       'SlimmeCheck.VALUE_DAYS_SINCE_LAST_DIENST_END',\n",
    "       'SlimmeCheck.VALUE_DAYS_SINCE_LAST_RELOCATION',\n",
    "       'SlimmeCheck.VALUE_AT_LEAST_ONE_ADDRESS_IN_AMSTERDAM',\n",
    "       'SlimmeCheck.VALUE_AFSPRAKEN_NO_SHOW_COUNT_LAST_YEAR',\n",
    "       'SlimmeCheck.VALUE_RECEIVED_SAME_PRODUCT_LAST_YEAR',\n",
    "       'SlimmeCheck.VALUE_DEELNAMES_STARTED_PERCENTAGE_LAST_YEAR',\n",
    "       'SlimmeCheck.VALUE_ACTIVE_ADDRESS_COUNT',\n",
    "       'SlimmeCheck.VALUE_HAS_PARTNER', \n",
    "       'SlimmeCheck.VALUE_HAS_MEDEBEWONER',\n",
    "       'SlimmeCheck.VALUE_AVG_PERCENTAGE_MAATREGEL',\n",
    "       'SlimmeCheck.VALUE_SUM_INKOMEN_BRUTO_VALUE',\n",
    "       'SlimmeCheck.VALUE_SUM_INKOMEN_BRUTO_WAS_MEAN_IMPUTED', \n",
    "       'Leeftijd<30',\n",
    "       'Leeftijd<40', \n",
    "       'Leeftijd<50', \n",
    "       'IsNederlands', \n",
    "       'IsWesters',\n",
    "       'Days_since_last_dienst_end_year', \n",
    "       'Days_since_last_dienst_end_months',\n",
    "       'Days_since_last_relocation', \n",
    "       'has_single_address',\n",
    "       'afspraken_no_show_count', \n",
    "       'afspraken_no_contact_count',\n",
    "       'resultaat_handhaving',\n",
    "       'is_parttime_parent', \n",
    "       'is_fulltime_parent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_training & df_prepilot columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training['onderzoekswaardig'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutations to df_training to align with df_prepilot\n",
    "df_training = df_training.rename(columns = {'onderzoekswaardig':'Label',\n",
    "                                            'GESLACHT':'geslacht'})\n",
    "df_training = df_training.drop(columns = ['DTGEBOORTE', 'is_onderzoek_hh'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutations to df_prepilot to align with df_training\n",
    "df_prepilot.loc[df_prepilot['geboortejaar'].isna(), 'geboortejaar'] = df_prepilot['geboortejaar'].mean()\n",
    "df_prepilot['geboortejaar'] = df_prepilot['geboortejaar'].astype(int)\n",
    "df_prepilot['dtaanvraag'] = pd.to_datetime(df_prepilot['dtaanvraag'])\n",
    "df_prepilot = df_prepilot.drop(columns = ['DTGEBOORTE', 'geboortejaar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We want everyones age on the dtaanvraag\n",
    "# # First give everyone a geboortedatum of 1st of July for their geboortejaar\n",
    "# # We don't know actual birthdays, but for our goal this is fine.\n",
    "# df_prepilot['geboortedatum'] = df_prepilot['geboortejaar'].apply(lambda year: pd.to_datetime(f'{year}-07-01'))\n",
    "# # Then we calculate age and store the result in a new column 'Leeftijd'\n",
    "# df_prepilot['Leeftijd'] = np.floor((df_prepilot['dtaanvraag'] - df_prepilot['geboortedatum']) / np.timedelta64(1, 'Y'))\n",
    "# # Then we can drop\n",
    "# df_prepilot = df_prepilot.drop(columns = ['geboortedatum', 'geboortejaar'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(df_training.columns).intersection(set(df_prepilot.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_training.columns).difference(set(df_prepilot.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_prepilot.columns).difference(set(df_training.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have almost the same columns for df_prepilot and df_training.\n",
    "Now filter out the ones that we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = ['Unnamed: 0',\n",
    "                'dtaanvraag',\n",
    "                'DIENSTNR',\n",
    "                'NATIONALITEIT1',\n",
    "                'model_prob',\n",
    "                'onderzoekswaardig']\n",
    "\n",
    "def drop_columns(df:pd.DataFrame, drop_columns:list = drop_columns) -> pd.DataFrame:\n",
    "    df = df.drop(columns = drop_columns, errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = drop_columns(df_training)\n",
    "df_prepilot = drop_columns(df_prepilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pilot & the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot[['onderzoekswaardig', 'Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot[['SlimmeCheck.SCORE', 'model_after_reweighing_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns for most important features df_pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib_columns = ['SlimmeCheck.CONTRIB_DEELNAMES_STARTED_PERCENTAGE_LAST_YEAR',\n",
    "       'SlimmeCheck.CONTRIB_AFSPRAKEN_NO_CONTACT_COUNT_LAST_YEAR',\n",
    "       'SlimmeCheck.CONTRIB_APPLIED_FOR_SAME_PRODUCT_LAST_YEAR',\n",
    "       'SlimmeCheck.CONTRIB_RECEIVED_SAME_PRODUCT_LAST_YEAR',\n",
    "       'SlimmeCheck.CONTRIB_TOTAL_VERMOGEN',\n",
    "       'SlimmeCheck.CONTRIB_DAYS_SINCE_LAST_DIENST_END',\n",
    "       'SlimmeCheck.CONTRIB_AT_LEAST_ONE_ADDRESS_IN_AMSTERDAM',\n",
    "       'SlimmeCheck.CONTRIB_AFSPRAKEN_NO_SHOW_COUNT_LAST_YEAR',\n",
    "       'SlimmeCheck.CONTRIB_DAYS_SINCE_LAST_RELOCATION',\n",
    "       'SlimmeCheck.CONTRIB_ACTIVE_ADDRESS_COUNT',\n",
    "       'SlimmeCheck.CONTRIB_HAS_PARTNER',\n",
    "       'SlimmeCheck.CONTRIB_HAS_MEDEBEWONER',\n",
    "       'SlimmeCheck.CONTRIB_AVG_PERCENTAGE_MAATREGEL',\n",
    "       'SlimmeCheck.CONTRIB_SUM_INKOMEN_BRUTO_VALUE',\n",
    "       'SlimmeCheck.CONTRIB_SUM_INKOMEN_BRUTO_WAS_MEAN_IMPUTED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the highest contributing features\n",
    "def n_highest(row, n):\n",
    "    row = row.abs()\n",
    "    sorted_row = sorted(row, reverse=True)\n",
    "    n_highest_value = sorted_row[n]\n",
    "    n_highest_column = row.index[row == n_highest_value][0]\n",
    "    return n_highest_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We want the highest contributing features\n",
    "# def n_highest(row, n):\n",
    "#     sorted_row = sorted(row, reverse=True)\n",
    "#     n_highest_value = sorted_row[n]\n",
    "#     n_highest_column = row.index[row == n_highest_value][0]  \n",
    "#     return n_highest_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_pilot[contrib_columns].iloc[0,:].abs(),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot['SlimmeCheck.CONTRIB_AFSPRAKEN_NO_SHOW_COUNT_LAST_YEAR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepilot['Belangrijkste feature 1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_pos_neg(value, df = df_pilot):\n",
    "#     if value > 0:\n",
    "#         return ' (+)'\n",
    "#     elif value < 0:\n",
    "#         return ' (-)'\n",
    "#     else:\n",
    "#         return ' (0)'\n",
    "\n",
    "# While we know the highest contributors, we still have to know if it was a positive or negative contribution to adhere to the style used in the pilot output    \n",
    "def check_pos_neg(value, df=df_pilot):\n",
    "    try:\n",
    "        col_value = df[value]\n",
    "        positive_mask = col_value > 0\n",
    "        negative_mask = col_value < 0\n",
    "        zero_mask = col_value == 0\n",
    "        \n",
    "        result = pd.Series(index=col_value.index, dtype=str)\n",
    "        result[positive_mask] = ' (+)'\n",
    "        result[negative_mask] = ' (-)'\n",
    "        result[zero_mask] = ' (0)'\n",
    "        \n",
    "        return result.fillna(' (Column Not Found)')\n",
    "    except KeyError:\n",
    "        return ' (Column Not Found)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "belangrijkste_feature_columnnames = ['Belangrijkste feature 1', 'Belangrijkste feature 2', 'Belangrijkste feature 3']\n",
    "\n",
    "for i, imp_feature in enumerate(belangrijkste_feature_columnnames):\n",
    "    df_pilot[imp_feature] = df_pilot[contrib_columns].apply(lambda row: n_highest(row, i), axis=1)\n",
    "    df_pilot[f\"{imp_feature}_pos_neg\"] = df_pilot[imp_feature].apply(check_pos_neg)[0]\n",
    "    # df_pilot[imp_feature] = f\"{df_pilot[imp_feature]}{(df_pilot[imp_feature].apply(lambda row: check_pos_neg(row, imp_feature)))}\"\n",
    "    \n",
    "    df_pilot[imp_feature] = df_pilot[imp_feature].apply(lambda x: x.split('SlimmeCheck.CONTRIB_')[1].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to rewrite the features\n",
    "feature_name_mapping = {\n",
    "    \"deelnames_started_percentage_last_year\": \"Percentage deelnames gestart\",\n",
    "    \"at_least_one_address_in_amsterdam\": \"Adres in Amsterdam\",\n",
    "    \"active_address_count\": \"Aantal adressen\",\n",
    "    \"days_since_last_relocation\": \"Dagen sinds verhuizing\",\n",
    "    \"days_since_last_dienst_end\": \"Dagen sinds einde dienst\",\n",
    "    \"has_medebewoner\": \"Medebewoner\",\n",
    "    \"avg_percentage_maatregel\": \"Gemiddelde percentage maatregel\",\n",
    "    \"total_vermogen\": \"Totaal vermogen\",\n",
    "    \"afspraken_no_show_count_last_year\": \"Aantal afspraken no show\",\n",
    "    \"has_partner\": \"Partner\",\n",
    "    \"sum_inkomen_bruto_was_mean_imputed\": \"Inkomen onbekend\",\n",
    "    \"applied_for_same_product_last_year\": \"Eerder Levensonderhoud aangevraagd\",\n",
    "    \"received_same_product_last_year\": \"Eerder Levensonderhoud ontvangen\",\n",
    "    \"afspraken_no_contact_count_last_year\": \"Aantal afspraken geen contact\",\n",
    "    \"sum_inkomen_bruto_value\": \"Totaal bruto inkomen\",\n",
    "}\n",
    "\n",
    "for imp_feature in belangrijkste_feature_columnnames:\n",
    "    df_pilot[imp_feature] = df_pilot[imp_feature].replace(feature_name_mapping)\n",
    "    df_pilot[imp_feature] = df_pilot[imp_feature] + df_pilot[f\"{imp_feature}_pos_neg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot['Belangrijkste feature 1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepilot['Belangrijkste feature 1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the columns of df_pilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_pilot_for_bias_analysis = ['Leeftijd', 'Bias.Nationaliteit', 'Bias.Geslacht', 'applied_for_same_product_last_year', 'total_vermogen',\n",
    "       'days_since_last_dienst_end', 'days_since_last_relocation',\n",
    "       'at_least_one_address_in_amsterdam',\n",
    "       'afspraken_no_show_count_last_year', 'received_same_product_last_year',\n",
    "       'deelnames_started_percentage_last_year', 'active_address_count',\n",
    "       'model_before_reweighing_score',\n",
    "       'model_before_reweighing_prediction',\n",
    "       'model_after_reweighing_score',\n",
    "       'model_after_reweighing_prediction',\n",
    "       'onderzoekswaardig',\n",
    "       'Belangrijkste feature 1', 'Belangrijkste feature 2', 'Belangrijkste feature 3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we filter for the columns that have the same info we have in df_prepilot and df_training\n",
    "# This is almost all data we need for the bias analysis\n",
    "df_pilot = df_pilot[columns_pilot_for_bias_analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot['Bias.Geslacht'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't have the data for these two columns for the pilot, so we add them as NaN\n",
    "df_pilot['is_parttime_parent'] = np.nan\n",
    "df_pilot['is_fulltime_parent'] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pilot = df_pilot.rename(columns = { 'Bias.Nationaliteit': 'NATIONALITEIT1_OMSCHRIJVING', \n",
    " 'Bias.Geslacht': 'geslacht', \n",
    " 'onderzoekswaardig': 'Label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to concat for easier manipulation but we want to keep info about the origin of the data\n",
    "df_pilot['dataset'] = 'Pilot'\n",
    "df_prepilot['dataset'] = 'Prepilot'\n",
    "# df_training['dataset'] = 'Training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df_pilot, df_prepilot, df_training])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "- Now we want the features that are necessary for the bias analysis.\n",
    "- We should have all the required information for this.\n",
    "- This section is partly based on the work of Tess for the bias analysis of the pilot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leeftijd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some NAN, we fill those with median as is described in the technical documentation\n",
    "df_concat.loc[df_concat['Leeftijd'].isna(), 'Leeftijd'] = df_concat['Leeftijd'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['Leeftijd'] = df_concat['Leeftijd'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate generic function to map age based on threshold\n",
    "def map_age(age, threshold_age):\n",
    "    if age < threshold_age:\n",
    "        return 1\n",
    "    elif age >= threshold_age:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three new columns in the dataset\n",
    "    # 1. Leeftijd<30, where 1 = 0-29 and 0 = 30+\n",
    "    # 2. Leeftijd<40, where 1 = 0-39 and 0 = 40+\n",
    "    # 3. Leeftijd<50, where 1 = 0-49 and 0 = 50+\n",
    "threshold_age = [30,40,50]\n",
    "\n",
    "for age in threshold_age:\n",
    "    df_concat[f'Leeftijd<{age}'] = df_concat['Leeftijd'].apply(map_age, args=(age,))\n",
    "    print(df_concat[f'Leeftijd<{age}'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nationaliteit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in feature Bias.Nationaliteit\n",
    "all_nationalities = df_concat['NATIONALITEIT1_OMSCHRIJVING'].unique()\n",
    "len(all_nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'IsNederlands' where: 0 = Niet-Nederlands, 1 = Nederlands, 2 = Onbekende Nationaliteit\n",
    "# Please note: It is decided to include the values: 'Onbekend' as unkown nationality (same as missing values)\n",
    "def is_nederlands(nationality):\n",
    "    if nationality == 'Nederlandse':\n",
    "        return 1\n",
    "    elif pd.isna(nationality):\n",
    "        return 2\n",
    "    elif nationality in [\"Onbekend\"]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_concat['IsNederlands'] = df_concat['NATIONALITEIT1_OMSCHRIJVING'].apply(is_nederlands)\n",
    "df_concat['IsNederlands'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json file where Western and Non-Western nationalities are defined\n",
    "# These files are the same as used during the pre-pilot bias analysis\n",
    "# This is based on Annex 1 in the pre-pilot Bias analysis documentation: https://algoritmeregister.amsterdam.nl/ai-system/onderzoekswaardigheid-slimme-check-levensonderhoud/1086/\n",
    "json_file_path = '\\data\\west-nonwest-nationalities.json'\n",
    "\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all nationalities in the dataset are categorized in one of the lists (western or non-western)\n",
    "all_nationalities_set =set(all_nationalities)\n",
    "western_set = set(data['west'])\n",
    "non_western_set = set(data['nonwest'])\n",
    "\n",
    "if all_nationalities_set.issubset(western_set.union(non_western_set)):\n",
    "    print('All nationalities in the dataset are categorized')\n",
    "else: \n",
    "    uncategorized = all_nationalities_set - western_set.union(non_western_set)\n",
    "    print(\"The following nationalities are not categorized:\")\n",
    "    for nationality in uncategorized:\n",
    "        print (nationality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column 'IsWesters', where 0 = Niet westerse nationaliteit, 1 = Westerse nationaliteit, 2 = Onbekende Nationaliteit\n",
    "def is_westers(nationality):\n",
    "    if nationality in western_set:\n",
    "        return 1\n",
    "    elif nationality in non_western_set:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "df_concat['IsWesters'] = df_concat['NATIONALITEIT1_OMSCHRIJVING'].apply(is_westers)\n",
    "df_concat['IsWesters'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geslacht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['geslacht'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have both M/V/O and 1/2/0 as sex categorization, make uniform\n",
    "dict_map_sex_code = {1 : 'M',\n",
    "                     2 : 'V',\n",
    "                     0 : 'O'}\n",
    "\n",
    "df_concat['geslacht'] = df_concat['geslacht'].replace(dict_map_sex_code)\n",
    "\n",
    "# There are some nan, let's call it Onbekend\n",
    "df_concat['geslacht'] = df_concat['geslacht'].fillna('Onbekend')\n",
    "\n",
    "# Check result \n",
    "expected_values =  {'M', 'V', 'O', 'Onbekend'}\n",
    "assert set(df_concat['geslacht'].unique()) == expected_values, \"geslacht not equal to expected values. Please, make sure to clean the dataset\"\n",
    "\n",
    "df_concat['geslacht'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.loc[df_concat['dataset'] == 'Pilot', 'geslacht'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Feature: days_since_last_dienst_end & days_since_last_relocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate generic function to map days_since_last_dienst_end and days_since_last_relocation based on threshold\n",
    "def map_days(days, threshold_days):\n",
    "    if days > threshold_days:\n",
    "        return 1\n",
    "    elif days <= threshold_days:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function map_days to create three new columns:\n",
    "    # 1. Days_since_last_dienst_end_year, where 1 = meer dan een jaar (>365), 0 = minder dan een jaar (<=365)\n",
    "    # 2. Days_since_last_dienst_end_months, where 1 = meer dan twee maanden (>60), 0 = minder dan 2 maanden (<=60)\n",
    "    # 3. Days_since_last_relocation, where 1 = meer dan een jaar (>365), 0 = minder dan een jaar (<=365)\n",
    "\n",
    "def calculate_days_since_last(df, input_col, output_col, threshold):\n",
    "    df[output_col] = df[input_col].apply(map_days, args=(threshold,))\n",
    "    return df[output_col].value_counts()\n",
    "\n",
    "dict_days_since_last = [{'input_col': 'days_since_last_dienst_end', 'output_col': 'Days_since_last_dienst_end_year', 'threshold':365}, \n",
    "                        {'input_col': 'days_since_last_dienst_end', 'output_col':'Days_since_last_dienst_end_months', 'threshold':60}, \n",
    "                        {'input_col': 'days_since_last_relocation', 'output_col':'Days_since_last_relocation', 'threshold':365}]\n",
    "\n",
    "for dictionary in dict_days_since_last:\n",
    "    days_value_counts = calculate_days_since_last(df_concat, dictionary['input_col'], dictionary['output_col'], dictionary['threshold'])\n",
    "    print(days_value_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adressen: active_address_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column 'has_single_address', where 0 = 2 or more addresses, 1 = one address, 2 = less than 1 address \n",
    "def address_count(address):\n",
    "    if address == 1:\n",
    "        return 1\n",
    "    elif address > 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "df_concat['has_single_address'] = df_concat['active_address_count'].apply(address_count)\n",
    "df_concat['has_single_address'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label: label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.loc[\n",
    "    ~(df_concat['is_fulltime_parent'].isna())\n",
    "    &\n",
    "    (df_concat['dataset'] == 'Prepilot')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have multiple ways of labeling, make uniform\n",
    "dict_map_label = {1 : 'Onderzoekswaardig',\n",
    "                    0 : 'Niet onderzoekswaardig'}\n",
    "\n",
    "df_concat['Label'] = df_concat['Label'].replace(dict_map_label)\n",
    "df_concat['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN to 'Onbekend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.fillna('Onbekend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final bias columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.to_excel('20240203_bias_lhr_processed.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
